# Layer Normalization in Transformers: A Concise Tutorial
Source: https://youtu.be/W9g7j22MO-Q?si=skHsewXJ60kNL3I6
## Why Normalization Matters

Training deep Transformers with billions of parameters faces critical challenges: vanishing/exploding gradients and unstable optimization. **Layer Normalization** solves these problems.

### The Unnormalized Data Problem

Consider predicting house prices using:
- Square footage: 500-5,000
- Number of rooms: 1-10

This scale disparity creates an **elongated cost function**â€”like a narrow canyon versus a smooth bowl.

<img width="259" height="150" alt="elongated_cost_fun" src="https://github.com/user-attachments/assets/745e7908-9868-44b6-8d58-66f51fa6c7a8" />


**Consequences:**
- High learning rate â†’ Overshooting
- Low learning rate â†’ Slow convergence  
- High variance â†’ Gradient instability

**Solution:** Normalization transforms data to have mean = 0, std = 1

```
Formula: z_norm = (z - Î¼) / Ïƒ

Example:
Original: [1000, 1500, 2000, 2500, 3000]
Î¼ = 2000, Ïƒ = 707.1
Normalized: [-1.41, -0.71, 0.00, 0.71, 1.41]
```

Result: Circular cost function â†’ faster, stable training.

<img width="150" height="150" alt="circular" src="https://github.com/user-attachments/assets/0f60078b-c04f-45c7-bd92-938e6e96745a" />


## Internal Covariate Shift: The Deep Network Challenge
<img width="515" height="84" alt="image" src="https://github.com/user-attachments/assets/32ab0c4a-3018-489b-aaec-05014c02ac0a" />

Normalizing only input isn't enough. In deep networks:
1. Weights update during backpropagation
2. Each layer's output distribution changes
3. Next layers receive a "moving target" every iteration

**Analogy:** Learning basketball where rules change dailyâ€”you never master fundamentals.

**Example:** Train on red roses â†’ test on white roses = failure (covariate shift). In deep networks, this happens **internally at every layer, every iteration**.

## Layer Normalization: The Two-Stage Solution

### Stage 1: Normalize

```
For data point with features [zâ‚, zâ‚‚, ..., z_d]:

Î¼ = (1/d) Î£(zâ±¼)
Ïƒ = sqrt((1/d) Î£(zâ±¼ - Î¼)Â²)
z_norm = (z - Î¼) / (Ïƒ + Îµ)
```

### Stage 2: Scale and Shift

```
z_output = Î³ âŠ™ z_norm + Î²

Î³ (gamma) = learnable scale (init: 1)
Î² (beta)  = learnable shift (init: 0)
```

**Why?** Flexibility! Not all layers need full normalization. The model **learns** optimal normalization through Î³ and Î².

**Extreme case:** If Î³ = Ïƒ and Î² = Î¼ â†’ recovers original values (model chose no normalization).

## Batch Norm vs. Layer Norm

### Batch Normalization
Normalizes **across batch** for each feature:

```
3 samples, 5 features:
     F1   F2   F3   F4   F5
S1: [2.1, 3.4, 1.8, 4.2, 2.9]
S2: [1.9, 3.1, 2.1, 4.5, 3.2]
S3: [2.3, 3.6, 1.7, 4.1, 2.8]
     â†“    â†“    â†“    â†“    â†“
Calculate Î¼, Ïƒ for each column
```

### Layer Normalization
Normalizes **across features** for each sample:

```
S1: [2.1, 3.4, 1.8, 4.2, 2.9] â†’ Î¼â‚, Ïƒâ‚
    â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
S2: [1.9, 3.1, 2.1, 4.5, 3.2] â†’ Î¼â‚‚, Ïƒâ‚‚
    â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Each row independent!
```

### The Padding Problem

**Why Transformers need Layer Norm:**

```
Batch: 2 sentences, max length 4, embedding 512
S1: "The cat sat" â†’ [embâ‚, embâ‚‚, embâ‚ƒ, PAD]
S2: "I love coding" â†’ [embâ‚, embâ‚‚, embâ‚ƒ, embâ‚„]

Batch Norm problem:
Token 4 normalization = mean([PAD_zeros, embâ‚„])
â†’ Zeros corrupt statistics! âœ—

Layer Norm solution:
Each token normalized across its 512 features
â†’ No cross-contamination! âœ“
```

## Application in Transformers

### Structure
**Input shape:** [batch_size, seq_length, embed_dim]  
Example: [32, 128, 768]

**For each token:**
```
Token: [eâ‚, eâ‚‚, ..., eâ‚‡â‚†â‚ˆ]

Î¼ = Î£(eáµ¢) / 768
Ïƒ = sqrt(Î£(eáµ¢ - Î¼)Â² / 768)
e_norm = (e - Î¼) / Ïƒ
e_output = Î³ Ã— e_norm + Î²

Note: Î³, Î² shape = [768] (one per feature)
```

### Transformer Block
```
Input â†’ Multi-Head Attention â†’ Add & LayerNorm
     â†’ Feed-Forward Network  â†’ Add & LayerNorm
     â†’ Output
```

**Modern models:** GPT-3 has 96 layers Ã— 2 LayerNorms = **192 normalization operations per forward pass!**

## Concrete Example

```
Token output: [0.8, -1.2, 0.5, 2.1, -0.3]

Step 1: Î¼ = 0.38, Ïƒ = 1.18

Step 2: Normalize
[0.8, -1.2, 0.5, 2.1, -0.3]
â†’ [0.36, -1.34, 0.10, 1.46, -0.58]

Step 3: Apply Î³=[1.5,...], Î²=[0.1,...]
â†’ [0.64, -1.91, 0.25, 2.29, -0.77]
```

## Pre-Norm vs. Post-Norm

**Post-Norm (Original):** LayerNorm after sublayer  
**Pre-Norm (Modern):** LayerNorm before sublayer

```
Pre-Norm (GPT):
Input â†’ LayerNorm â†’ Attention â†’ Add
     â†’ LayerNorm â†’ FFN â†’ Add
```

**Why Pre-Norm?** More stable for 100+ layer networks, easier training.

## Key Takeaways

1. **Stabilizes training** by preventing gradient issues and enabling faster convergence
2. **Internal covariate shift** requires normalizing intermediate layers
3. **Layer Norm > Batch Norm** for Transformers (handles variable lengths/padding)
4. **Scale/shift (Î³, Î²)** give models learned flexibility
5. **Essential for modern AI:** Every major Transformer relies on LayerNorm
6. **Without it:** Training 100+ layer Transformers would be impossible

## Practical Impact

**GPT-3:** 175B parameters, 96 layers, 192 LayerNorm ops/token  
**Training cost:** $4-12 million

Layer Normalization ensures gradients flow cleanly through this massive network. Without it, the investment would be wasted on unstable training.

---

**Bottom Line:** Layer Normalization transformed Transformers from theory to the foundation of modern AI. Simple concept, profound impact! ðŸš€
